{
 "cells": [
  {
   "cell_type": "raw",
   "id": "fa270394",
   "metadata": {},
   "source": [
    "question 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffe9b88",
   "metadata": {},
   "source": [
    "A linear Support Vector Machine (SVM) is a binary classification algorithm that seeks to find the hyperplane that maximizes the margin between two classes in a linearly separable dataset. The decision function of a linear SVM is defined as:\n",
    "\n",
    "\\[f(x) = \\mathbf{w} \\cdot \\mathbf{x} + b\\]\n",
    "\n",
    "Where:\n",
    "- \\(f(x)\\) represents the decision function.\n",
    "- \\(\\mathbf{w}\\) is the weight vector.\n",
    "- \\(\\mathbf{x}\\) is the input vector.\n",
    "- \\(b\\) is the bias term.\n",
    "\n",
    "The classification is determined by the sign of \\(f(x)\\). If \\(f(x) \\geq 0\\), then the sample is classified as one class, and if \\(f(x) < 0\\), it's classified as the other class.\n",
    "\n",
    "In this context, \\(\\mathbf{w}\\) and \\(b\\) are learned during the training process. The goal of the SVM algorithm is to find the optimal values of \\(\\mathbf{w}\\) and \\(b\\) that minimize the classification error and maximize the margin between the classes.\n",
    "\n",
    "The margin is defined as the distance between the hyperplane and the nearest data point from either class. Mathematically, the margin is \\(\\frac{2}{\\|\\mathbf{w}\\|}\\), where \\(\\|\\mathbf{w}\\|\\) represents the Euclidean norm (or length) of the weight vector.\n",
    "\n",
    "The objective function for training a linear SVM can be formulated as a convex optimization problem:\n",
    "\n",
    "\\[\n",
    "\\min_{\\mathbf{w}, b} \\frac{1}{2}\\|\\mathbf{w}\\|^2 \\quad \\text{subject to} \\quad y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 \\quad \\forall i\n",
    "\\]\n",
    "\n",
    "Here, \\(\\mathbf{x}_i\\) represents the training data, \\(y_i\\) is the corresponding class label (either +1 or -1), and the constraint ensures that all data points are correctly classified.\n",
    "\n",
    "The optimization problem seeks to find the hyperplane parameters \\(\\mathbf{w}\\) and \\(b\\) that minimize the \\(\\|\\mathbf{w}\\|^2\\) term (which relates to the margin) subject to the constraint that all data points are correctly classified.\n",
    "\n",
    "This formulation makes use of the concept of \"slack variables\" to handle cases where the data is not perfectly separable. In practice, this allows for a certain degree of misclassification to find a hyperplane that balances margin size and classification accuracy."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3506e018",
   "metadata": {},
   "source": [
    "question 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08193d98",
   "metadata": {},
   "source": [
    "The objective function of a linear Support Vector Machine (SVM) is formulated as a convex optimization problem. The goal is to find the hyperplane parameters that maximize the margin between the classes while minimizing the classification error.\n",
    "\n",
    "The objective function for a linear SVM can be written as:\n",
    "\n",
    "\\[ \\min_{\\mathbf{w}, b} \\frac{1}{2}\\|\\mathbf{w}\\|^2 \\]\n",
    "\n",
    "subject to the constraints:\n",
    "\n",
    "\\[ y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 \\quad \\text{for all training samples } i \\]\n",
    "\n",
    "Here, the terms represent:\n",
    "\n",
    "- \\(\\mathbf{w}\\) is the weight vector.\n",
    "- \\(b\\) is the bias term.\n",
    "- \\(\\mathbf{x}_i\\) represents the training data.\n",
    "- \\(y_i\\) is the corresponding class label, either +1 or -1.\n",
    "\n",
    "The objective is to minimize \\(\\frac{1}{2}\\|\\mathbf{w}\\|^2\\), which is equivalent to minimizing the squared Euclidean norm (length) of the weight vector. This term is related to the margin of the hyperplane.\n",
    "\n",
    "The constraints \\(y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1\\) ensure that all training samples are classified correctly. Specifically, they enforce that the data points are on the correct side of the decision boundary, taking into account the margin.\n",
    "\n",
    "This optimization problem aims to find the hyperplane parameters \\(\\mathbf{w}\\) and \\(b\\) that strike a balance between maximizing the margin (which is proportional to \\(\\frac{1}{\\|\\mathbf{w}\\|}\\)) and minimizing the classification error. This results in a hyperplane that provides a good generalization to unseen data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ccb2c09",
   "metadata": {},
   "source": [
    "question 03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5227e5c",
   "metadata": {},
   "source": [
    "The kernel trick is a fundamental concept in Support Vector Machine (SVM) theory that allows SVMs to efficiently handle non-linearly separable datasets. It does this by implicitly transforming the original input space into a higher-dimensional feature space, where the data may become linearly separable.\n",
    "\n",
    "In the original linear SVM, the decision boundary is a hyperplane that separates classes in the input space. However, in many real-world scenarios, the data may not be linearly separable. The kernel trick provides a way to effectively deal with such cases.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. **Mapping to a Higher-Dimensional Space**: The kernel trick avoids explicitly calculating and storing the coordinates of data points in this higher-dimensional space. Instead, it defines a kernel function, which computes the dot product of the transformed vectors without explicitly performing the transformation.\n",
    "\n",
    "   Mathematically, given two input vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), the kernel function is denoted as \\(K(\\mathbf{x}, \\mathbf{y})\\) and computes \\(\\phi(\\mathbf{x}) \\cdot \\phi(\\mathbf{y})\\), where \\(\\phi\\) is the transformation function.\n",
    "\n",
    "2. **Mercer's Condition**: The kernel function must satisfy Mercer's condition, which ensures that the kernel represents a valid inner product in some (possibly infinite-dimensional) feature space. This ensures that the SVM optimization problem remains convex, allowing for efficient training.\n",
    "\n",
    "3. **Efficient Computation**: The kernel trick allows SVMs to compute the decision function in terms of the kernel function, without explicitly transforming the data. This means that you can work with a potentially infinite-dimensional feature space efficiently.\n",
    "\n",
    "Commonly used kernels include:\n",
    "\n",
    "- **Linear Kernel**: \\(K(\\mathbf{x}, \\mathbf{y}) = \\mathbf{x} \\cdot \\mathbf{y}\\), equivalent to the original linear SVM.\n",
    "- **Polynomial Kernel**: \\(K(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x} \\cdot \\mathbf{y} + r)^d\\), where \\(r\\) is a user-defined constant and \\(d\\) is the degree of the polynomial.\n",
    "- **Radial Basis Function (RBF) or Gaussian Kernel**: \\(K(\\mathbf{x}, \\mathbf{y}) = \\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{y}\\|^2}{2\\sigma^2}\\right)\\), where \\(\\sigma\\) is a user-defined parameter.\n",
    "\n",
    "The choice of kernel and its hyperparameters is crucial in achieving good performance. Different kernels are suitable for different types of datasets, and finding the right one often involves experimentation and cross-validation."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2251294",
   "metadata": {},
   "source": [
    "question 04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da32618d",
   "metadata": {},
   "source": [
    "In a Support Vector Machine (SVM), support vectors play a crucial role in determining the decision boundary. They are the data points that are closest to the decision boundary and have the potential to influence the position and orientation of the hyperplane.\n",
    "\n",
    "Here's a detailed explanation with an example:\n",
    "\n",
    "**Example: Binary Classification with a Linear SVM**\n",
    "\n",
    "Let's consider a simple example of classifying points in a 2D plane into two classes, labeled as +1 and -1.\n",
    "\n",
    "Suppose we have the following points:\n",
    "\n",
    "Class +1 (blue): (1, 2), (2, 3), (3, 3)\n",
    "Class -1 (red): (3, 1), (4, 2), (5, 3)\n",
    "\n",
    "**Step 1: Training the SVM**\n",
    "\n",
    "1. **Plotting the Data**:\n",
    "   ![SVM Example](https://i.imgur.com/BVw2ilT.png)\n",
    "\n",
    "2. **Finding the Hyperplane**:\n",
    "   - In a linear SVM, the goal is to find the hyperplane that maximizes the margin (distance) between the two classes.\n",
    "   - The hyperplane can be represented as \\(\\mathbf{w} \\cdot \\mathbf{x} + b = 0\\), where \\(\\mathbf{w}\\) is the weight vector and \\(b\\) is the bias term.\n",
    "\n",
    "3. **Finding Support Vectors**:\n",
    "   - Support vectors are the data points closest to the decision boundary.\n",
    "   - In this case, they are the points (2, 3) and (4, 2) from classes +1 and -1, respectively.\n",
    "\n",
    "4. **Defining the Margin**:\n",
    "   - The margin is the perpendicular distance from the hyperplane to the nearest support vector. It is proportional to \\(\\frac{1}{\\|\\mathbf{w}\\|}\\).\n",
    "\n",
    "**Step 2: Role of Support Vectors**\n",
    "\n",
    "1. **Influence on Hyperplane**:\n",
    "   - The position and orientation of the hyperplane are determined by the support vectors.\n",
    "   - Changing other data points (those farther from the boundary) does not affect the hyperplane.\n",
    "\n",
    "2. **Robustness to Outliers**:\n",
    "   - Support vectors are critical for the SVM's ability to handle outliers. \n",
    "   - Even if there are misclassified or noisy points, as long as they are not support vectors, they won't significantly affect the hyperplane.\n",
    "\n",
    "3. **Efficient Storage and Computation**:\n",
    "   - Since the SVM decision function relies only on the support vectors, it's memory-efficient and computationally efficient. \n",
    "   - You don't need to store and process all training samples, only the support vectors.\n",
    "\n",
    "In this example, the decision boundary is determined by the support vectors (points (2, 3) and (4, 2)). The margin is the distance from the hyperplane to these support vectors. This allows the SVM to generalize well to new, unseen data.\n",
    "\n",
    "Overall, support vectors play a crucial role in defining the optimal hyperplane and, consequently, in achieving a good classification performance."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc4d6320",
   "metadata": {},
   "source": [
    "question 05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ce34b",
   "metadata": {},
   "source": [
    "Certainly! I'll illustrate the concepts of Hyperplane, Marginal Plane, Soft Margin, and Hard Margin in SVM with examples and corresponding graphs.\n",
    "\n",
    "### 1. Hyperplane:\n",
    "In a linear SVM, a hyperplane is a decision boundary that separates classes. For a 2D space, it's a line; for a 3D space, it's a plane, and so on. The hyperplane is defined by the weights (\\(\\mathbf{w}\\)) and bias (\\(b\\)) parameters.\n",
    "\n",
    "**Example:**\n",
    "Consider a 2D dataset with two classes, labeled as +1 and -1. The hyperplane equation is \\(w_1x_1 + w_2x_2 + b = 0\\), where \\((w_1, w_2)\\) are weights and \\(b\\) is the bias term.\n",
    "\n",
    "Graph:\n",
    "![Hyperplane](https://i.imgur.com/ikKnr4a.png)\n",
    "\n",
    "### 2. Marginal Plane:\n",
    "The marginal plane is a parallel plane to the hyperplane that is equidistant from it. The distance between the hyperplane and the marginal plane is the margin. It is defined by the support vectors.\n",
    "\n",
    "**Example:**\n",
    "Using the same 2D dataset, the marginal planes are the dashed lines parallel to the hyperplane.\n",
    "\n",
    "Graph:\n",
    "![Marginal Plane](https://i.imgur.com/EOWC5ji.png)\n",
    "\n",
    "### 3. Soft Margin:\n",
    "A soft margin allows some misclassification of training data to achieve a wider margin. This is useful when the data is not perfectly separable.\n",
    "\n",
    "**Example:**\n",
    "Suppose we have a dataset with a noisy point. With a soft margin, the algorithm might tolerate this misclassification.\n",
    "\n",
    "Graph:\n",
    "![Soft Margin](https://i.imgur.com/rbimT0g.png)\n",
    "\n",
    "### 4. Hard Margin:\n",
    "A hard margin SVM enforces strict classification. It requires that all training data be correctly classified, which can be problematic for noisy or overlapping data.\n",
    "\n",
    "**Example:**\n",
    "If we have a dataset where a hard margin is applied, it will try to find a hyperplane that perfectly separates the classes.\n",
    "\n",
    "Graph:\n",
    "![Hard Margin](https://i.imgur.com/Wg2DvPn.png)\n",
    "\n",
    "In practice, the choice between a soft margin (allowing misclassifications) and a hard margin (no misclassifications) depends on the nature of the data. A soft margin is generally more robust to noisy or overlapping data, but it requires tuning a parameter (C) that controls the trade-off between margin size and misclassification.\n",
    "\n",
    "Remember, these visualizations are simplified for conceptual understanding. In real-world scenarios, the dimensionality of the data can be much higher, making it impossible to visualize directly. The SVM algorithm efficiently works in high-dimensional spaces."
   ]
  },
  {
   "cell_type": "raw",
   "id": "96604b14",
   "metadata": {},
   "source": [
    "question 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45c5f54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of sets:\n",
      "X_train shape: (120, 4), y_train shape: (120,)\n",
      "X_test shape: (30, 4), y_test shape: (30,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now, you have the following variables available:\n",
    "# X_train: Training features\n",
    "# y_train: Corresponding labels for the training set\n",
    "# X_test: Testing features\n",
    "# y_test: Corresponding labels for the testing set\n",
    "\n",
    "# Optionally, you can print the shapes of the sets to verify the split\n",
    "print(\"Shapes of sets:\")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35ace75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Linear SVM classifier: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize a Linear SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the testing set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of the Linear SVM classifier: {accuracy*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e25b2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the testing set: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming 'y_test' contains the true labels and 'y_pred' contains the predicted labels\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy on the testing set: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc792ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Select the first two features for visualization\n",
    "X_subset = X_train[:, :2]\n",
    "\n",
    "# Define the meshgrid for plotting decision boundaries\n",
    "x_min, x_max = X_subset[:, 0].min() - 1, X_subset[:, 0].max() + 1\n",
    "y_min, y_max = X_subset[:, 1].min() - 1, X_subset[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "\n",
    "# Get the decision function values for the meshgrid\n",
    "Z = svm_classifier.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundaries and scatter plot of data points\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot decision boundaries\n",
    "plt.contour(xx, yy, Z, levels=[-1, 0, 1], colors='k', linestyles=['--', '-', '--'], alpha=0.5)\n",
    "\n",
    "# Scatter plot of data points\n",
    "plt.scatter(X_subset[:, 0], X_subset[:, 1], c=y_train, cmap=plt.cm.Paired, edgecolors='k')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Add legend for class labels\n",
    "plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='b', markersize=10, label='Class 0'),\n",
    "                    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='r', markersize=10, label='Class 1'),\n",
    "                    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='g', markersize=10, label='Class 2')],\n",
    "           loc='upper right')\n",
    "\n",
    "plt.title(\"Decision Boundaries of Linear SVM\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35280911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with C=0.1: 100.00%\n",
      "Accuracy with C=1: 100.00%\n",
      "Accuracy with C=10: 96.67%\n",
      "Accuracy with C=100: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define a list of different values for C\n",
    "C_values = [0.1, 1, 10, 100]\n",
    "\n",
    "for C_value in C_values:\n",
    "    # Initialize a Linear SVM classifier with specified C\n",
    "    svm_classifier = SVC(kernel='linear', C=C_value, random_state=42)\n",
    "    \n",
    "    # Train the classifier on the training set\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict labels for the testing set\n",
    "    y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy of the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy with C={C_value}: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e84214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
